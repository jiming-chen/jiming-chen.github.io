<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Jiming Chen</title>
    <link>https://jiming-chen.github.io/posts/</link>
    <description>Recent content in Posts on Jiming Chen</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Tue, 01 Jul 2025 01:28:34 -0700</lastBuildDate>
    <atom:link href="https://jiming-chen.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lazy Caching</title>
      <link>https://jiming-chen.github.io/posts/lazycaching/</link>
      <pubDate>Tue, 01 Jul 2025 01:28:34 -0700</pubDate>
      <guid>https://jiming-chen.github.io/posts/lazycaching/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s say you have an app where users can search up any book. From the user side, it might seem like we need a database row for every single edition, with title, publisher, description, cover image, and so on. Especially for descriptions and cover images, when considering up to 50 million editions, costs can accrue quickly, not to mention acquiring such a database would be difficult.&lt;/p&gt;
&lt;p&gt;A solution which I am quite partial to is what is known in system design as lazy caching. What this means is that we only load something in the first time it is needed and we don&amp;rsquo;t have it, and for subsequent requests, since we already have it, we don&amp;rsquo;t need to load it again. What I find elegant about it is that for the cost of one user having slightly higher latency, database size is much smaller, it allows the database to be built over time rather than all at once, and most importantly, to the user it appears as if the whole database is present.&lt;/p&gt;</description>
    </item>
    <item>
      <title>JIYP: Lessons Learned, Part 1</title>
      <link>https://jiming-chen.github.io/posts/jiyp1/</link>
      <pubDate>Mon, 30 Jun 2025 09:30:29 -0500</pubDate>
      <guid>https://jiming-chen.github.io/posts/jiyp1/</guid>
      <description>&lt;p&gt;Two weeks ago, I started working at a startup called Jarvis In Your Pocket. Imagine if you could track your nutritional (and overall) health using just an LLM, an LLM that could learn about you, be proactive, and give solid advice based on trusted expert sources selected by your doctor.&lt;/p&gt;
&lt;p&gt;Some things were easier than I expected. First, we envisioned a virtual retrieval-augmented generation (RAG) system whereby we could consult with multiple doctors who had their own trusted materials and RAG on a specific subset of all the sources based on which doctor had referred the user. While it may sound like we have to create virtual vector databases on top of the large corpus of content, it&amp;rsquo;s actually as simple filtering on chunk metadata like so:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
