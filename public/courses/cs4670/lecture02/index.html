<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 2: Image Filtering | Jiming Chen</title>
<meta name=keywords content><meta name=description content="Images
How do you represent a photograph? You could say pixels, but originally, there were no pixels in photographs. Therefore, we could say an image is a function that maps spatial location to intensity, where intensity is a number between 0 and 1, inclusive:
$$f : \mathbb{R}^2 \to [0, 1].$$

Fig. 1. An example of an image.
What about color images? We could extend the function definition to one using RGB:"><meta name=author content="Jiming Chen"><link rel=canonical href=https://jiming-chen.github.io/courses/cs4670/lecture02/><link crossorigin=anonymous href=/assets/css/stylesheet.862d2ab5d22734725b747026f6cbcd72cf56acbcb80e88f95d22ca71d767e619.css integrity="sha256-hi0qtdInNHJbdHAm9svNcs9WrLy4Doj5XSLKcddn5hk=" rel="preload stylesheet" as=style><link rel=icon href=https://jiming-chen.github.io/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jiming-chen.github.io/icons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jiming-chen.github.io/icons/favicon-32x32.png><link rel=apple-touch-icon href=https://jiming-chen.github.io/icons/apple-touch-icon.png><link rel=mask-icon href=https://jiming-chen.github.io/%3C/icons/apple-touch-icon.png%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jiming-chen.github.io/courses/cs4670/lecture02/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://jiming-chen.github.io/courses/cs4670/lecture02/"><meta property="og:site_name" content="Jiming Chen"><meta property="og:title" content="Lecture 2: Image Filtering"><meta property="og:description" content="Images How do you represent a photograph? You could say pixels, but originally, there were no pixels in photographs. Therefore, we could say an image is a function that maps spatial location to intensity, where intensity is a number between 0 and 1, inclusive:
$$f : \mathbb{R}^2 \to [0, 1].$$
Fig. 1. An example of an image. What about color images? We could extend the function definition to one using RGB:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="courses"><meta property="article:published_time" content="2025-01-24T13:26:11-05:00"><meta property="article:modified_time" content="2025-01-24T13:26:11-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 2: Image Filtering"><meta name=twitter:description content="Images
How do you represent a photograph? You could say pixels, but originally, there were no pixels in photographs. Therefore, we could say an image is a function that maps spatial location to intensity, where intensity is a number between 0 and 1, inclusive:
$$f : \mathbb{R}^2 \to [0, 1].$$

Fig. 1. An example of an image.
What about color images? We could extend the function definition to one using RGB:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Course Notes","item":"https://jiming-chen.github.io/courses/"},{"@type":"ListItem","position":2,"name":"CS 4670: Intro to Computer Vision","item":"https://jiming-chen.github.io/courses/cs4670/"},{"@type":"ListItem","position":3,"name":"Lecture 2: Image Filtering","item":"https://jiming-chen.github.io/courses/cs4670/lecture02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 2: Image Filtering","name":"Lecture 2: Image Filtering","description":"Images How do you represent a photograph? You could say pixels, but originally, there were no pixels in photographs. Therefore, we could say an image is a function that maps spatial location to intensity, where intensity is a number between 0 and 1, inclusive:\n$$f : \\mathbb{R}^2 \\to [0, 1].$$\nFig. 1. An example of an image. What about color images? We could extend the function definition to one using RGB:\n","keywords":[],"articleBody":"Images How do you represent a photograph? You could say pixels, but originally, there were no pixels in photographs. Therefore, we could say an image is a function that maps spatial location to intensity, where intensity is a number between 0 and 1, inclusive:\n$$f : \\mathbb{R}^2 \\to [0, 1].$$\nFig. 1. An example of an image. What about color images? We could extend the function definition to one using RGB:\n$$f : \\mathbb{R}^2 \\to [0,1]^3.$$\nThis definition is great by itself, but computers cannot represent arbitrary functions. Therefore, digital images record only intensities/colors at a finite grid of locations. Thus, grayscale images are 2D arrays while color images are 3D arrays. On the same note, pixel values are not real numbers but rather integers between 0 and 255 (but this is not always the case).\nImage Processing One task we might want to complete is to convert a picture of a notebook into a text document. This requires us to recognize each letter, but how do we know where the letters are?\nRecall that images are functions. thus, grayscale images are functions $f : [a,b] \\times [c,d] \\to \\mathbb{R}$. Therefore, we can use a threshold:\n$$f’(x,y) = \\begin{cases} 1 \u0026 f(x,y) \u003e 122 \\\\ 0 \u0026 f(x,y) \\leq 122. \\end{cases}$$\nBlurring Suppose we take a photo in the dark. How do we fix this? By adding a constant to the function, the image gets brighter, but there still isn’t much contrast.\nTo increase contrast, we could multiply $f$ by a constant, but this increases noise:\nFig. 2. A noisy image. First, we can assume that noise is independent of other locations and distributed according to a Gaussian distribution. Technically, low-light photography is not, but that is another story.\nWe can also use a nice fact about the real world: close areas in real life have similar intensity values. Therefore, we can replace each pixel’s value with the average of its neighbors. You can choose neighborhoods of different sizes, e.g. $3\\times3$ or $5\\times5$. However, neighborhoods that are too big can cross boundaries, and you can lose information despite getting rid of more noise. This technique is called mean filtering. A $3\\times3$ mean filter would look like\n$$S[f](m,n) = \\sum_{i=-1}^1 \\sum_{j=-1}^1 f(m+i,n+j)/9.$$\nMean filtering only slightly changes the appearance of noisy images, but it can be impactful when thresholding.\nWe can add a weight and generalize the neighborhood size:\n$$S[f](m,n) = \\sum_{i=-k}^k \\sum_{j=-k}^k w(i,j) f(m+i,n+j).$$\nHere, the weight $w$ accounts for the actually division in the mean. For a mean filter, $w(i,j) = 1/(2k+1)^2$.\nIf $w(i,j) \\geq 0$ and sum to 1, then we have a weighted mean filter. On the other hand, if we let $w(i,j$) be arbitrary real numbers, then there becomes a correlation between certain areas of different neighborhoods, which we call cross-correlation.\nWhat happens at the image boundary? At the top left pixel, can cannot take an average including a pixel above and to the left of that pixel since such a pixel doesn’t exist.\nOne approach we could take is to not compute an average for boundary pixels, shrinking the new image. This is called valid cross-correlation, and the output image has size $m-k+1$.\nAlternatively, we could treat non-existent pixels as having a certain value, like 0, and maintain the original size. This is called same cross-correlation. However, we could also compute averages for pixels outside of the original image, making the new image larger. This is called full cross-correlation and results in an output image of size $m+k-1$.\nConvolution and Cross-Correlation Cross-correlation is defined as\n$$w \\otimes f (m,n) = S[f](m,n) = \\sum_{i=-k}^k \\sum_{j=-k}^k w(i,j) f(m+i,n+j),$$\nwhile convolution is defined as\n$$w * f(m, n) = S[f](m,n) = \\sum_{i=-k}^k \\sum_{j=-k}^k w(i,j) f(m-i,n-j),$$\nImportantly, cross-correlation (and convolution) is linear in both $w$ and $f$.\nAdditionally, cross-correlation follows shift equivariance, so shifting then filtering is the same as filtering then shifting.\nCross-correlation and convolution are similar operations and have similar properties, but they are just weighted differently.\nSharpening When we blur images, we are changing the values of each pixel. If we undid those changes on the original picture, we could get a sharper image:\n$$f_{sharp} = f + \\alpha(f-f_{blur}).$$\nCan we make a filter to do this? Simplifying the above gives\n$$\\begin{aligned} f_{sharp} \u0026= f + \\alpha(f-f_{blur}) \\\\ \u0026= (1+\\alpha)f-\\alpha f_{blur} \\\\ \u0026= (1+\\alpha)(w*f) - \\alpha(v * f) \\\\ \u0026= ((1+\\alpha)w - \\alpha v) * f, \\end{aligned}$$\nwhere $w$ is the identity filter and $v$ is a blurring filter. The last line follows from linearity.\n","wordCount":"749","inLanguage":"en","datePublished":"2025-01-24T13:26:11-05:00","dateModified":"2025-01-24T13:26:11-05:00","author":{"@type":"Person","name":"Jiming Chen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jiming-chen.github.io/courses/cs4670/lecture02/"},"publisher":{"@type":"Organization","name":"Jiming Chen","logo":{"@type":"ImageObject","url":"https://jiming-chen.github.io/icons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jiming-chen.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://jiming-chen.github.io/icons/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jiming-chen.github.io/about title="About Me"><span>About Me</span></a></li><li><a href=https://jiming-chen.github.io/projects/ title=Portfolio><span>Portfolio</span></a></li><li><a href=https://jiming-chen.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://jiming-chen.github.io/courses/ title="Course Notes"><span>Course Notes</span></a></li><li><a href=https://jiming-chen.github.io/piano/ title=Piano><span>Piano</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Lecture 2: Image Filtering</h1><div class=post-meta><span title='2025-01-24 13:26:11 -0500 EST'>January 24, 2025</span>&nbsp;·&nbsp;Jiming Chen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#images>Images</a></li><li><a href=#image-processing>Image Processing</a><ul><li><a href=#blurring>Blurring</a></li><li><a href=#convolution-and-cross-correlation>Convolution and Cross-Correlation</a></li><li><a href=#sharpening>Sharpening</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=images>Images<a hidden class=anchor aria-hidden=true href=#images>#</a></h2><p>How do you represent a photograph? You could say pixels, but originally, there were no pixels in photographs. Therefore, we could say an image is a function that maps spatial location to intensity, where intensity is a number between 0 and 1, inclusive:</p><p>$$f : \mathbb{R}^2 \to [0, 1].$$</p><img src=/courses/cs4670/lecture02/train.jpg alt="Derailed train" width=50% class=center><figcaption>Fig. 1. An example of an image.</figcaption><p>What about color images? We could extend the function definition to one using RGB:</p><p>$$f : \mathbb{R}^2 \to [0,1]^3.$$</p><p>This definition is great by itself, but computers cannot represent arbitrary functions. Therefore, <em>digital</em> images record only intensities/colors at a finite grid of locations. Thus, grayscale images are 2D arrays while color images are 3D arrays. On the same note, pixel values are not real numbers but rather integers between 0 and 255 (but this is not always the case).</p><h2 id=image-processing>Image Processing<a hidden class=anchor aria-hidden=true href=#image-processing>#</a></h2><p>One task we might want to complete is to convert a picture of a notebook into a text document. This requires us to recognize each letter, but how do we know where the letters are?</p><p>Recall that images are functions. thus, grayscale images are functions $f : [a,b] \times [c,d] \to \mathbb{R}$. Therefore, we can use a threshold:</p><p>$$f&rsquo;(x,y) = \begin{cases}
1 & f(x,y) > 122 \\
0 & f(x,y) \leq 122.
\end{cases}$$</p><h3 id=blurring>Blurring<a hidden class=anchor aria-hidden=true href=#blurring>#</a></h3><p>Suppose we take a photo in the dark. How do we fix this? By adding a constant to the function, the image gets brighter, but there still isn&rsquo;t much contrast.</p><p>To increase contrast, we could multiply $f$ by a constant, but this increases noise:</p><img src=/courses/cs4670/lecture02/noise.png alt="Derailed train" width=70% class=center><figcaption>Fig. 2. A noisy image.</figcaption><p>First, we can assume that noise is independent of other locations and distributed according to a Gaussian distribution. Technically, low-light photography is not, but that is another story.</p><p>We can also use a nice fact about the real world: close areas in real life have similar intensity values. Therefore, we can replace each pixel&rsquo;s value with the average of its neighbors. You can choose neighborhoods of different sizes, e.g. $3\times3$ or $5\times5$. However, neighborhoods that are too big can cross boundaries, and you can lose information despite getting rid of more noise. This technique is called <strong>mean filtering</strong>. A $3\times3$ mean filter would look like</p><p>$$S[f](m,n) = \sum_{i=-1}^1 \sum_{j=-1}^1 f(m+i,n+j)/9.$$</p><p>Mean filtering only slightly changes the appearance of noisy images, but it can be impactful when thresholding.</p><p>We can add a weight and generalize the neighborhood size:</p><p>$$S[f](m,n) = \sum_{i=-k}^k \sum_{j=-k}^k w(i,j) f(m+i,n+j).$$</p><p>Here, the weight $w$ accounts for the actually division in the mean. For a mean filter, $w(i,j) = 1/(2k+1)^2$.</p><p>If $w(i,j) \geq 0$ and sum to 1, then we have a weighted mean filter. On the other hand, if we let $w(i,j$) be <em>arbitrary real numbers</em>, then there becomes a correlation between certain areas of different neighborhoods, which we call <strong>cross-correlation</strong>.</p><p>What happens at the image boundary? At the top left pixel, can cannot take an average including a pixel above and to the left of that pixel since such a pixel doesn&rsquo;t exist.</p><p>One approach we could take is to not compute an average for boundary pixels, shrinking the new image. This is called <strong>valid cross-correlation</strong>, and the output image has size $m-k+1$.</p><p>Alternatively, we could treat non-existent pixels as having a certain value, like 0, and maintain the original size. This is called <strong>same cross-correlation</strong>. However, we could also compute averages for pixels outside of the original image, making the new image larger. This is called <strong>full cross-correlation</strong> and results in an output image of size $m+k-1$.</p><h3 id=convolution-and-cross-correlation>Convolution and Cross-Correlation<a hidden class=anchor aria-hidden=true href=#convolution-and-cross-correlation>#</a></h3><p>Cross-correlation is defined as</p><p>$$w \otimes f (m,n) = S[f](m,n) = \sum_{i=-k}^k \sum_{j=-k}^k w(i,j) f(m+i,n+j),$$</p><p>while convolution is defined as</p><p>$$w * f(m, n) = S[f](m,n) = \sum_{i=-k}^k \sum_{j=-k}^k w(i,j) f(m-i,n-j),$$</p><p>Importantly, cross-correlation (and convolution) is <strong>linear</strong> in both $w$ and $f$.</p><p>Additionally, cross-correlation follows <strong>shift equivariance</strong>, so shifting then filtering is the same as filtering then shifting.</p><p>Cross-correlation and convolution are similar operations and have similar properties, but they are just weighted differently.</p><h3 id=sharpening>Sharpening<a hidden class=anchor aria-hidden=true href=#sharpening>#</a></h3><p>When we blur images, we are changing the values of each pixel. If we undid those changes on the original picture, we could get a sharper image:</p><p>$$f_{sharp} = f + \alpha(f-f_{blur}).$$</p><p>Can we make a filter to do this? Simplifying the above gives</p><p>$$\begin{aligned}
f_{sharp} &= f + \alpha(f-f_{blur}) \\
&= (1+\alpha)f-\alpha f_{blur} \\
&= (1+\alpha)(w*f) - \alpha(v * f) \\
&= ((1+\alpha)w - \alpha v) * f,
\end{aligned}$$</p><p>where $w$ is the identity filter and $v$ is a blurring filter. The last line follows from linearity.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://jiming-chen.github.io/courses/phys2217/lecture02/><span class=title>Next »</span><br><span>Lecture 2: Charge and Coulomb's Law</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://jiming-chen.github.io/>Jiming Chen</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>