<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CS 4670: Intro to Computer Vision | Jiming Chen</title><meta name=keywords content><meta name=description content="Taken SP25. Taught by Bharath Hariharan and Wei-Chiu Ma."><meta name=author content="Jiming Chen"><link rel=canonical href=https://jiming-chen.github.io/courses/cs4670/><link crossorigin=anonymous href=/assets/css/stylesheet.0788e016f920508aef4b1850b916d4a2a5a89167bb733638fd43d8d85a28d7f4.css integrity="sha256-B4jgFvkgUIrvSxhQuRbUoqWokWe7czY4/UPY2Foo1/Q=" rel="preload stylesheet" as=style><link rel=icon href=https://jiming-chen.github.io/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jiming-chen.github.io/icons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jiming-chen.github.io/icons/favicon-32x32.png><link rel=apple-touch-icon href=https://jiming-chen.github.io/icons/apple-touch-icon.png><link rel=mask-icon href=https://jiming-chen.github.io/%3C/icons/apple-touch-icon.png%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://jiming-chen.github.io/courses/cs4670/index.xml><link rel=alternate hreflang=en href=https://jiming-chen.github.io/courses/cs4670/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KTD621L9T6"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KTD621L9T6")</script><meta property="og:url" content="https://jiming-chen.github.io/courses/cs4670/"><meta property="og:site_name" content="Jiming Chen"><meta property="og:title" content="CS 4670: Intro to Computer Vision"><meta property="og:description" content="Taken SP25. Taught by Bharath Hariharan and Wei-Chiu Ma."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="CS 4670: Intro to Computer Vision"><meta name=twitter:description content="Jiming Chen's Personal Website"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Course Notes","item":"https://jiming-chen.github.io/courses/"},{"@type":"ListItem","position":2,"name":"CS 4670: Intro to Computer Vision","item":"https://jiming-chen.github.io/courses/cs4670/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jiming-chen.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://jiming-chen.github.io/icons/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jiming-chen.github.io/about title="About Me"><span>About Me</span></a></li><li><a href=https://jiming-chen.github.io/projects/ title=Portfolio><span>Portfolio</span></a></li><li><a href=https://jiming-chen.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://jiming-chen.github.io/courses/ title="Course Notes"><span>Course Notes</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>CS 4670: Intro to Computer Vision
<a href=/courses/cs4670/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><div class=post-content><p>Taken SP25. Taught by Bharath Hariharan and Wei-Chiu Ma.</p></div><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 23: Recognition</h2></header><div class=entry-content><p>What if you want information about images? This can be done using machine learning. Specifically, classification of images is useful, and we need classification datasets to train models.
MNIST was the first major dataset and consisted of handwritten numbers. Caltech 101 consisted of objects, but there were spurious correlations. ImageNet contains lots of labeled images.</p></div><footer class=entry-footer><span title='2025-04-07 14:06:41 -0400 EDT'>April 7, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 23: Recognition" href=https://jiming-chen.github.io/courses/cs4670/lecture23/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 21: Other Methods of Obtaining 3D Structure</h2></header><div class=entry-content><p>So far, when talking about 3D structure from images, we have been talking about the depth of each pixel $D[\mathbf{p}] \in \mathbb{R}^+$. However, we can also look at the surface normal at any point because up close, any object approximates a plane. We can let this information be conveyed in another channel $N[\mathbf{p}] \in \mathbb{S}^2$.
Sometimes, when things in images are too far, the depth channel can have less information, but we can use surface normal to fill in missing information to complete the depth map.
...</p></div><footer class=entry-footer><span title='2025-03-26 13:36:50 -0400 EDT'>March 26, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 21: Other Methods of Obtaining 3D Structure" href=https://jiming-chen.github.io/courses/cs4670/lecture21/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 20 : Recovering Fundamental Matrix</h2></header><div class=entry-content><p>Structure From Motion If we have two cameras whose parameters we don’t know, can we recover camera location and orientation without a calibration object in order to recover the 3D world?
Recall that $\mathbf{q}^TF\mathbf{p} = 0$. In order to find $F$, we can use an 8-point algorithm.
Suppose $\mathbf{p} = \begin{bmatrix}x_1 \\ y_1 \\ 1\end{bmatrix}$ and $\mathbf{q} = \begin{bmatrix}x_2 \\ y_2 \\ 1\end{bmatrix}$ correspond to the same 3D point. Then, we have a linear system of equations with the entries of $F$ as coefficients.
...</p></div><footer class=entry-footer><span title='2025-03-17 13:36:16 -0400 EDT'>March 17, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 20 : Recovering Fundamental Matrix" href=https://jiming-chen.github.io/courses/cs4670/lecture20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 18: Epipolar Geometry</h2></header><div class=entry-content><p>Given two camera centers and a point in 3D space, we can define the plane containing these points. We call this the epipolar plane. The epipolar line for a camera is the intersection of the epipolar plane and the camera plane.
We can also define the epipolar pencil, which is the family of planes formed by changing the point in 3D space.
For the math, we assume the intrinsic parameters are the identity matrix, and we assume the world coordinate system is centered at the 1st camera pinhole with the $Z$ axis along the viewing direction. Then,
...</p></div><footer class=entry-footer><span title='2025-03-12 13:45:41 -0400 EDT'>March 12, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 18: Epipolar Geometry" href=https://jiming-chen.github.io/courses/cs4670/lecture18/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 17: Planar Homography</h2></header><div class=entry-content><p>Since we can set the world coordinates ourselves, we can let the world $Z$ coordinate be $0$, discarding the third column of the matrix $P$, resulting in a matrix we call $H$.
$H$ has $9$ entries, but accounting for $\lambda$, there are only $8$ parameters. Thus, we need four points to determine $H$.
Homography works for image stitching because it helps us map different camera angles onto the same coordinate plane.</p></div><footer class=entry-footer><span title='2025-03-10 13:30:23 -0400 EDT'>March 10, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 17: Planar Homography" href=https://jiming-chen.github.io/courses/cs4670/lecture17/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 16: Binocular Stereo</h2></header><div class=entry-content><p>It is hard to do 3D reconstruction with a single image because obviously we have lost information about depth. However, if we have two cameras, we can use parallax to triangulate.
Assume we have two calibrated cameras and we have already run correspondence to find a pair of matching points:
$$\begin{aligned} \begin{bmatrix} x_1 \\ y_1 \\ 1 \end{bmatrix} &\equiv P^{(1)}\begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} \\ \begin{bmatrix} x_2 \\ y_2 \\ 2 \end{bmatrix} &\equiv P^{(2)}\begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} \end{aligned} $$
...</p></div><footer class=entry-footer><span title='2025-03-07 13:26:08 -0500 EST'>March 7, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 16: Binocular Stereo" href=https://jiming-chen.github.io/courses/cs4670/lecture16/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 15: Camera Calibration</h2></header><div class=entry-content><p>Recap of Aligning Coordinate Systems Recall that $\mathbf{x}_c = R\mathbf{x}_w + \mathbf{t}$. We first find $R$:
$$R = \begin{bmatrix}X_c & Y_c & Z_c\end{bmatrix}^T.$$
Once we find this, if we let $\mathbf{c}$ be the location of the camera, then
$$R\mathbf{c} + \mathbf{t} = 0 \implies \mathbf{t} = -R\mathbf{c}.$$
Then, the extrinsic (world to camera) matrix is
$$\begin{bmatrix} R & \mathbf{t} \\ \mathbf{0}^T & 1 \end{bmatrix}.$$
Overall, we have
$$ \begin{bmatrix} u \\ v \\ 1\end{bmatrix} \equiv \begin{bmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} f & 0 & 0 & 0 \\ 0 & f & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} R & \mathbf{t} \\ \mathbf{0}^T & 1 \end{bmatrix} \begin{bmatrix} U \\ V \\ W \\ 1\end{bmatrix}, $$
...</p></div><footer class=entry-footer><span title='2025-03-03 13:29:10 -0500 EST'>March 3, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 15: Camera Calibration" href=https://jiming-chen.github.io/courses/cs4670/lecture15/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 14: More 3D Geometry</h2></header><div class=entry-content><p>How do we go from the film plane (projected image) to pixel array? Recall that for the latter, we place the origin in the top left. However, for the former, the origin is in the middle. Thus, to go from the former to the latter, we need to include an offset:
$$u = f\frac{X}{Z} + o_x, \quad v = f\frac{Y}{Z} +o_y.$$
This assumes that the focal length $f$ is the same in each direction. In practice, this may not be true, so we add scaling factors
...</p></div><footer class=entry-footer><span title='2025-02-28 13:32:01 -0500 EST'>February 28, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 14: More 3D Geometry" href=https://jiming-chen.github.io/courses/cs4670/lecture14/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 13: Coordinate System Transformations</h2></header><div class=entry-content><p>Recall our projection equations
$$x = \frac{X}{Z}, \quad y = \frac{Y}{Z}.$$
However, because we have division, this is not linear. How can we represent it like
$$\mathbf{x}_c = R\mathbf{x}_w + \mathbf{t}?$$
This brings us to homogeneous coordinates, whereby 2D points are actually 3D points and 3D points are actually 4D points. For example, we can write
$$(x,y) = (x,y,1) = (kx, ky, k).$$
This allows us to write the transformation above as
...</p></div><footer class=entry-footer><span title='2025-02-26 13:39:12 -0500 EST'>February 26, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 13: Coordinate System Transformations" href=https://jiming-chen.github.io/courses/cs4670/lecture13/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 12: 3D Coordinates</h2></header><div class=entry-content><p>Say we have a line passing through a point $A$ with direction $D$. Then, we can write
$$Q(\lambda) = A + \lambda D.$$
Then, we can have a parallel line
$$R(\lambda) = B + \lambda D.$$
Then, as $\lambda \to \infty$, the projections of both lines go to $(D_X/D_Z, D_Y/D_Z)$.
If $D_Z = 0$, it means the lines lie parallel to the $xy$-plane.
Additionally, we can take the limit of planes. Given
...</p></div><footer class=entry-footer><span title='2025-02-24 13:29:07 -0500 EST'>February 24, 2025</span>&nbsp;·&nbsp;Jiming Chen</footer><a class=entry-link aria-label="post link to Lecture 12: 3D Coordinates" href=https://jiming-chen.github.io/courses/cs4670/lecture12/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://jiming-chen.github.io/courses/cs4670/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://jiming-chen.github.io/>Jiming Chen</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>