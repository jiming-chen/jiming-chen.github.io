<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CS 4670: Intro to Computer Vision on Jiming Chen</title>
    <link>https://jiming-chen.github.io/courses/cs4670/</link>
    <description>Recent content in CS 4670: Intro to Computer Vision on Jiming Chen</description>
    <generator>Hugo -- 0.144.1</generator>
    <language>en</language>
    <lastBuildDate>Mon, 24 Feb 2025 13:29:07 -0500</lastBuildDate>
    <atom:link href="https://jiming-chen.github.io/courses/cs4670/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lecture 12: 3D Coordinates</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture12/</link>
      <pubDate>Mon, 24 Feb 2025 13:29:07 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture12/</guid>
      <description>&lt;p&gt;Say we have a line passing through a point $A$ with direction $D$. Then, we can write&lt;/p&gt;
&lt;p&gt;$$Q(\lambda) = A + \lambda D.$$&lt;/p&gt;
&lt;p&gt;Then, we can have a parallel line&lt;/p&gt;
&lt;p&gt;$$R(\lambda) = B + \lambda D.$$&lt;/p&gt;
&lt;p&gt;Then, as $\lambda \to \infty$, the projections of both lines go to $(D_X/D_Z, D_Y/D_Z)$.&lt;/p&gt;
&lt;p&gt;If $D_Z = 0$, it means the lines lie parallel to the $xy$-plane.&lt;/p&gt;
&lt;p&gt;Additionally, we can take the limit of planes. Given&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lecture 11: Correspondences and Reconstruction</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture11/</link>
      <pubDate>Fri, 21 Feb 2025 13:27:21 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture11/</guid>
      <description>&lt;h2 id=&#34;correspondences-wrap-up&#34;&gt;Correspondences Wrap-Up&lt;/h2&gt;
&lt;p&gt;If we have a bunch of corners in two images, how we do match them to each other? Since each corner is described by a vector, we can compute Euclidean distance and correspond to each corner in image 1 the closest corner in image 2.&lt;/p&gt;
&lt;p&gt;However, not every corner in image 1 is guaranteed to have a matching corner in image 2. Therefore, we devise a method of scoring correspondences.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lecture 10: Feature Descriptors</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture10/</link>
      <pubDate>Wed, 19 Feb 2025 13:27:22 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture10/</guid>
      <description>&lt;p&gt;A feature descriptor is a way to describe pixels using vectors in order to do correspondence.&lt;/p&gt;
&lt;p&gt;One example is the &lt;em&gt;Multiscale Oriented Patches&lt;/em&gt; (MOPS) descriptor, which describes a corner by the patch around the pixel. The patch should have scale, rotation, and photometric invariance.&lt;/p&gt;
&lt;p&gt;Automatic scale selection is done by taking the function at many scales and taking the scale which maximizes the function. This implemented using the Gaussian pre-filtering and blurring (Gaussian pyramid) that we used for zooming in on images and computing $f$ for a fixed window size.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lecture 6: Edge Detection and Clustering</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture06/</link>
      <pubDate>Wed, 05 Feb 2025 13:31:49 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture06/</guid>
      <description>&lt;h2 id=&#34;edge-detection&#34;&gt;Edge Detection&lt;/h2&gt;
&lt;p&gt;Recall that we can use anisotropic filtering because Gaussian filtering should not be the same in every direction. One variant of the anisotropic Gaussian is the Sobel filter:&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} 1 \\ 2 \\ 1\end{bmatrix} * \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; -1 \end{bmatrix} = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; -1 \\
2 &amp;amp; 0 &amp;amp; -2 \\
1 &amp;amp; 0 &amp;amp; -1
\end{bmatrix}.$$&lt;/p&gt;
&lt;p&gt;This filter first highlights differences between dark and light areas but then does a convolution in the $x$ direction.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lecture 5: Grouping and Edges</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture05/</link>
      <pubDate>Fri, 31 Jan 2025 13:28:07 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture05/</guid>
      <description>&lt;h2 id=&#34;bilinear-interpolation-review&#34;&gt;Bilinear Interpolation Review&lt;/h2&gt;
&lt;p&gt;To perform bilinear interpolation, we could use the traditional area formula, or we could use one linear interpolation to find points on the edge that lie across from each other through the point of interest and perform another interpolation to get the value of the point we want.&lt;/p&gt;
&lt;h2 id=&#34;grouping-and-edges&#34;&gt;Grouping and Edges&lt;/h2&gt;
&lt;p&gt;Why grouping? As humans, when we see images or moving pictures, we don&amp;rsquo;t look at pixels, we see objects that the pixels represent. Therefore, it is useful to have computers do the same thing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lecture 4: Convolution and Geometric Transformations</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture04/</link>
      <pubDate>Wed, 29 Jan 2025 13:28:09 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture04/</guid>
      <description>&lt;h2 id=&#34;convolution&#34;&gt;Convolution&lt;/h2&gt;
&lt;p&gt;Last class, we mentioned that we can use convolution to turn color images gray. To do this, we can treat a color image as three layers of grayscale images and use a three-layer kernel.&lt;/p&gt;
&lt;p&gt;We also mentioned looking at 1D convolution as matrix multiplication. To extend this to 2 dimensions, we can simply put all the rows/columns of the image in one long column vector, doing the matrix multiplication, and unwrapping the long column vector back into an image. Although it requires thinking to squish the kernel and place it into the square matrix, it works out.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lecture 3: More Convolution</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture03/</link>
      <pubDate>Mon, 27 Jan 2025 13:29:37 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture03/</guid>
      <description>&lt;h2 id=&#34;convolution&#34;&gt;Convolution&lt;/h2&gt;
&lt;p&gt;The below filter for a pixel if the pixels to the bottom left are light and the pixels to the top right are dark, so it detects edges.&lt;/p&gt;
&lt;img src=&#34;https://jiming-chen.github.io/courses/cs4670/lecture03/edge.png&#34; alt=&#34;Edge filter&#34; width=&#34;80%&#34; class=&#34;center&#34;&gt;
&lt;figcaption&gt;Fig. 1. An edge-detecting filter.&lt;/figcaption&gt;
&lt;p&gt;Shift equivariance is an important property, and it is why convolution is so used in the real world (including in machine learning). If we have a filter that detects cats, it should not break if the cat moves.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lecture 2: Image Filtering</title>
      <link>https://jiming-chen.github.io/courses/cs4670/lecture02/</link>
      <pubDate>Fri, 24 Jan 2025 13:26:11 -0500</pubDate>
      <guid>https://jiming-chen.github.io/courses/cs4670/lecture02/</guid>
      <description>&lt;h2 id=&#34;images&#34;&gt;Images&lt;/h2&gt;
&lt;p&gt;How do you represent a photograph? You could say pixels, but originally, there were no pixels in photographs. Therefore, we could say an image is a function that maps spatial location to intensity, where intensity is a number between 0 and 1, inclusive:&lt;/p&gt;
&lt;p&gt;$$f : \mathbb{R}^2 \to [0, 1].$$&lt;/p&gt;
&lt;img src=&#34;https://jiming-chen.github.io/courses/cs4670/lecture02/train.jpg&#34; alt=&#34;Derailed train&#34; width=&#34;50%&#34; class=&#34;center&#34;&gt;
&lt;figcaption&gt;Fig. 1. An example of an image.&lt;/figcaption&gt;
&lt;p&gt;What about color images? We could extend the function definition to one using RGB:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
