<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 10: Feature Descriptors | Jiming Chen</title>
<meta name=keywords content><meta name=description content="A feature descriptor is a way to describe pixels using vectors in order to do correspondence.
One example is the Multiscale Oriented Patches (MOPS) descriptor, which describes a corner by the patch around the pixel. The patch should have scale, rotation, and photometric invariance.
Automatic scale selection is done by taking the function at many scales and taking the scale which maximizes the function. This implemented using the Gaussian pre-filtering and blurring (Gaussian pyramid) that we used for zooming in on images and computing $f$ for a fixed window size."><meta name=author content="Jiming Chen"><link rel=canonical href=https://jiming-chen.github.io/courses/cs4670/lecture10/><link crossorigin=anonymous href=/assets/css/stylesheet.862d2ab5d22734725b747026f6cbcd72cf56acbcb80e88f95d22ca71d767e619.css integrity="sha256-hi0qtdInNHJbdHAm9svNcs9WrLy4Doj5XSLKcddn5hk=" rel="preload stylesheet" as=style><link rel=icon href=https://jiming-chen.github.io/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jiming-chen.github.io/icons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jiming-chen.github.io/icons/favicon-32x32.png><link rel=apple-touch-icon href=https://jiming-chen.github.io/icons/apple-touch-icon.png><link rel=mask-icon href=https://jiming-chen.github.io/%3C/icons/apple-touch-icon.png%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jiming-chen.github.io/courses/cs4670/lecture10/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KTD621L9T6"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KTD621L9T6")</script><meta property="og:url" content="https://jiming-chen.github.io/courses/cs4670/lecture10/"><meta property="og:site_name" content="Jiming Chen"><meta property="og:title" content="Lecture 10: Feature Descriptors"><meta property="og:description" content="A feature descriptor is a way to describe pixels using vectors in order to do correspondence.
One example is the Multiscale Oriented Patches (MOPS) descriptor, which describes a corner by the patch around the pixel. The patch should have scale, rotation, and photometric invariance.
Automatic scale selection is done by taking the function at many scales and taking the scale which maximizes the function. This implemented using the Gaussian pre-filtering and blurring (Gaussian pyramid) that we used for zooming in on images and computing $f$ for a fixed window size."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="courses"><meta property="article:published_time" content="2025-02-19T13:27:22-05:00"><meta property="article:modified_time" content="2025-02-19T13:27:22-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 10: Feature Descriptors"><meta name=twitter:description content="A feature descriptor is a way to describe pixels using vectors in order to do correspondence.
One example is the Multiscale Oriented Patches (MOPS) descriptor, which describes a corner by the patch around the pixel. The patch should have scale, rotation, and photometric invariance.
Automatic scale selection is done by taking the function at many scales and taking the scale which maximizes the function. This implemented using the Gaussian pre-filtering and blurring (Gaussian pyramid) that we used for zooming in on images and computing $f$ for a fixed window size."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Course Notes","item":"https://jiming-chen.github.io/courses/"},{"@type":"ListItem","position":2,"name":"CS 4670: Intro to Computer Vision","item":"https://jiming-chen.github.io/courses/cs4670/"},{"@type":"ListItem","position":3,"name":"Lecture 10: Feature Descriptors","item":"https://jiming-chen.github.io/courses/cs4670/lecture10/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 10: Feature Descriptors","name":"Lecture 10: Feature Descriptors","description":"A feature descriptor is a way to describe pixels using vectors in order to do correspondence.\nOne example is the Multiscale Oriented Patches (MOPS) descriptor, which describes a corner by the patch around the pixel. The patch should have scale, rotation, and photometric invariance.\nAutomatic scale selection is done by taking the function at many scales and taking the scale which maximizes the function. This implemented using the Gaussian pre-filtering and blurring (Gaussian pyramid) that we used for zooming in on images and computing $f$ for a fixed window size.\n","keywords":[],"articleBody":"A feature descriptor is a way to describe pixels using vectors in order to do correspondence.\nOne example is the Multiscale Oriented Patches (MOPS) descriptor, which describes a corner by the patch around the pixel. The patch should have scale, rotation, and photometric invariance.\nAutomatic scale selection is done by taking the function at many scales and taking the scale which maximizes the function. This implemented using the Gaussian pre-filtering and blurring (Gaussian pyramid) that we used for zooming in on images and computing $f$ for a fixed window size.\nFor rotation invariance, we can again look at the second moment matrix:\n$$\\begin{bmatrix} \\sum I_x^2 \u0026 \\sum I_xI_y \\\\ \\sum I_xI_y \u0026 \\sum I_y^2 \\end{bmatrix}.$$\nThe eigenvectors of this matrix should tell us the orientation of the image.\nPhotometric invariance can be done very easily. If the image is brighter, then we can the mean from all values, and if the image has higher contrast, then we can divide by the standard deviation.\nMOPS is invariant to additive and multiplicative changes to intensity, but sometimes lighting changes can be more sophisticated, such as day versus night. Therefore, we worry less about the precise color values and more about the edges.\nAdditionally, MOPS is only invariant to rotation and translation, but not shearing, which is a very common transformation because it results from any change in view angle.\nThe solution is to have an edge descriptor which groups objects into different buckets, which allows invariance to small deformations. This is called quantized orientation (quantization is also used in machine learning).\nWe can do this with histograms. Importantly, with histograms, you don’t know which things go toward the count, just what the count is. Therefore, we can break an object into a grid and in each grid, we keep track of the number of pixels in each grid in each bucket. We can also use a weighted histogram.\nFor this descriptor (the scale invariant feature transform), we still want scale and rotation invariance as in the MOPS descriptor. SIFT is extremely well-engineered, so it is hard to find a deep learning solution which beats SIFT.\nIn SIFT, rather than using the second moment matrix, it uses the mode of the histogram (dominant orientation) to match orientations.\nThere are several things we have to watch out for when using SIFT.\nOne trick that you can use is to only keep edges of high gradient magnitude, which we do because there are a lot of edge pixles with low gradient magnitudes, which are extremely noisy.\nAnother issue we could encounter is a pixel switching bins after a transformation. We can fix this by allowing a pixel to vote for its two nearest neighbors (linear interpolation).\nWe are using gradient magnitudes, but gradient magnitudes are still affected by multiplicative intensity changes, so we still have to normalize that.\nThere could be multiple modes when finding the dominant orientation, in which case we have to check all possibilities.\n","wordCount":"492","inLanguage":"en","datePublished":"2025-02-19T13:27:22-05:00","dateModified":"2025-02-19T13:27:22-05:00","author":{"@type":"Person","name":"Jiming Chen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jiming-chen.github.io/courses/cs4670/lecture10/"},"publisher":{"@type":"Organization","name":"Jiming Chen","logo":{"@type":"ImageObject","url":"https://jiming-chen.github.io/icons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jiming-chen.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://jiming-chen.github.io/icons/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jiming-chen.github.io/about title="About Me"><span>About Me</span></a></li><li><a href=https://jiming-chen.github.io/projects/ title=Portfolio><span>Portfolio</span></a></li><li><a href=https://jiming-chen.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://jiming-chen.github.io/courses/ title="Course Notes"><span>Course Notes</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Lecture 10: Feature Descriptors</h1><div class=post-meta><span title='2025-02-19 13:27:22 -0500 EST'>February 19, 2025</span>&nbsp;·&nbsp;Jiming Chen</div></header><div class=post-content><p>A feature descriptor is a way to describe pixels using vectors in order to do correspondence.</p><p>One example is the <em>Multiscale Oriented Patches</em> (MOPS) descriptor, which describes a corner by the patch around the pixel. The patch should have scale, rotation, and photometric invariance.</p><p>Automatic scale selection is done by taking the function at many scales and taking the scale which maximizes the function. This implemented using the Gaussian pre-filtering and blurring (Gaussian pyramid) that we used for zooming in on images and computing $f$ for a fixed window size.</p><p>For rotation invariance, we can again look at the second moment matrix:</p><p>$$\begin{bmatrix}
\sum I_x^2 & \sum I_xI_y \\
\sum I_xI_y & \sum I_y^2
\end{bmatrix}.$$</p><p>The eigenvectors of this matrix should tell us the orientation of the image.</p><p>Photometric invariance can be done very easily. If the image is brighter, then we can the mean from all values, and if the image has higher contrast, then we can divide by the standard deviation.</p><p>MOPS is invariant to additive and multiplicative changes to intensity, but sometimes lighting changes can be more sophisticated, such as day versus night. Therefore, we worry less about the precise color values and more about the edges.</p><p>Additionally, MOPS is only invariant to rotation and translation, but not shearing, which is a very common transformation because it results from any change in view angle.</p><p>The solution is to have an edge descriptor which groups objects into different buckets, which allows invariance to small deformations. This is called <em>quantized orientation</em> (quantization is also used in machine learning).</p><p>We can do this with histograms. Importantly, with histograms, you don&rsquo;t know which things go toward the count, just what the count is. Therefore, we can break an object into a grid and in each grid, we keep track of the number of pixels in each grid in each bucket. We can also use a weighted histogram.</p><p>For this descriptor (the <em>scale invariant feature transform</em>), we still want scale and rotation invariance as in the MOPS descriptor. SIFT is extremely well-engineered, so it is hard to find a deep learning solution which beats SIFT.</p><p>In SIFT, rather than using the second moment matrix, it uses the mode of the histogram (dominant orientation) to match orientations.</p><p>There are several things we have to watch out for when using SIFT.</p><ul><li><p>One trick that you can use is to only keep edges of high gradient magnitude, which we do because there are a lot of edge pixles with low gradient magnitudes, which are extremely noisy.</p></li><li><p>Another issue we could encounter is a pixel switching bins after a transformation. We can fix this by allowing a pixel to vote for its two nearest neighbors (linear interpolation).</p></li><li><p>We are using gradient magnitudes, but gradient magnitudes are still affected by multiplicative intensity changes, so we still have to normalize that.</p></li><li><p>There could be multiple modes when finding the dominant orientation, in which case we have to check all possibilities.</p></li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://jiming-chen.github.io/>Jiming Chen</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>