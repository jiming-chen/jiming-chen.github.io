<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 2: The Evolution and Architecture of Modern Computers | Jiming Chen</title><meta name=keywords content><meta name=description content="Modern CPU architecture has many cores on one chip, each of which has their own cache. There is also off-chip memory. Nvidia GPUs have many cores, and we can think about them virtually in 3D shapes (?? idk, Birman yaps).
Moore&rsquo;s Law says that because of transistor size decreases, we can get exponential speedups. But around 2006-2009, it seemed to be ending because faster chips require faster clock and can fry."><meta name=author content="Jiming Chen"><link rel=canonical href=https://jiming-chen.github.io/courses/cs4414/lecture02/><link crossorigin=anonymous href=/assets/css/stylesheet.d20d31710e1e17ede7c7f9ededb138159f9228abc86b338e8d1053ed139865b7.css integrity="sha256-0g0xcQ4eF+3nx/nt7bE4FZ+SKKvIazOOjRBT7ROYZbc=" rel="preload stylesheet" as=style><link rel=icon href=https://jiming-chen.github.io/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jiming-chen.github.io/icons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jiming-chen.github.io/icons/favicon-32x32.png><link rel=apple-touch-icon href=https://jiming-chen.github.io/icons/apple-touch-icon.png><link rel=mask-icon href=https://jiming-chen.github.io/%3C/icons/apple-touch-icon.png%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jiming-chen.github.io/courses/cs4414/lecture02/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KTD621L9T6"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KTD621L9T6")</script><meta property="og:url" content="https://jiming-chen.github.io/courses/cs4414/lecture02/"><meta property="og:site_name" content="Jiming Chen"><meta property="og:title" content="Lecture 2: The Evolution and Architecture of Modern Computers"><meta property="og:description" content="Modern CPU architecture has many cores on one chip, each of which has their own cache. There is also off-chip memory. Nvidia GPUs have many cores, and we can think about them virtually in 3D shapes (?? idk, Birman yaps).
Moore’s Law says that because of transistor size decreases, we can get exponential speedups. But around 2006-2009, it seemed to be ending because faster chips require faster clock and can fry."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="courses"><meta property="article:published_time" content="2025-08-28T15:09:04-04:00"><meta property="article:modified_time" content="2025-08-28T15:09:04-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 2: The Evolution and Architecture of Modern Computers"><meta name=twitter:description content="Modern CPU architecture has many cores on one chip, each of which has their own cache. There is also off-chip memory. Nvidia GPUs have many cores, and we can think about them virtually in 3D shapes (?? idk, Birman yaps).
Moore&rsquo;s Law says that because of transistor size decreases, we can get exponential speedups. But around 2006-2009, it seemed to be ending because faster chips require faster clock and can fry."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Course Notes","item":"https://jiming-chen.github.io/courses/"},{"@type":"ListItem","position":2,"name":"CS 4114: Systems Programming","item":"https://jiming-chen.github.io/courses/cs4414/"},{"@type":"ListItem","position":3,"name":"Lecture 2: The Evolution and Architecture of Modern Computers","item":"https://jiming-chen.github.io/courses/cs4414/lecture02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 2: The Evolution and Architecture of Modern Computers","name":"Lecture 2: The Evolution and Architecture of Modern Computers","description":"Modern CPU architecture has many cores on one chip, each of which has their own cache. There is also off-chip memory. Nvidia GPUs have many cores, and we can think about them virtually in 3D shapes (?? idk, Birman yaps).\nMoore\u0026rsquo;s Law says that because of transistor size decreases, we can get exponential speedups. But around 2006-2009, it seemed to be ending because faster chips require faster clock and can fry.\n","keywords":[],"articleBody":"Modern CPU architecture has many cores on one chip, each of which has their own cache. There is also off-chip memory. Nvidia GPUs have many cores, and we can think about them virtually in 3D shapes (?? idk, Birman yaps).\nMoore’s Law says that because of transistor size decreases, we can get exponential speedups. But around 2006-2009, it seemed to be ending because faster chips require faster clock and can fry.\nHowever, we started using parallelism (many CPUs with lower clock speeds).\nA computer needs nearby memory, but applications need access to “all” the memory. This is why we get “non-uniform memory access” behavior (NUMA). With NUMA, we seem to have been able to keep up with Moore’s Law.\nGene Ahmdahl researched parallelism and asked how fast we could do things with infinite parallelism. He realized that sequential bottlenecks become a huge issue, similar to how driving in a fast car doesn’t matter if you’re stuck behind a tractor. Ahmdal said that if $p$ is the percentage of the task that can be parallelized, then the maximum possible speed up is $1/(1-p)$ (this just says we cannot speed up sequential tasks).\nWhen we write code with NUMA, nothing changes about what we actually write, but since each NUMA CPU (pair) has allocated DRAM, only a small portion of the memory is fast.\nThis results in a 4-tiered hierarchy for data access: registers, local DRAM, remote DRAM, and off-chip memory.\nNUMA “pretends” to be old architecture by abstracting away architectural differences, but ML often does not know about this, resulting in unoptimized code. Similar behavior also occurs in GPUs such as Nvidia’s H100.\nWhy are Python and Java expensive?\nPython is interpreted and cpompiles down to a high-level representation. But this interpreter can make a lot of bad decisions for you.\nJava compiles twice (to byte code then via JIT). This compiled code can be very fast, but sometimes, the compiler has to do a lot of type checking (at compile-time and runtime) because of dynamic types and polymorphism. Additionally, everything is an object, which hsa a lot of overhead.\nIn C++, all objects are compile-time, so there is no type-checking at runtime.\n","wordCount":"362","inLanguage":"en","datePublished":"2025-08-28T15:09:04-04:00","dateModified":"2025-08-28T15:09:04-04:00","author":{"@type":"Person","name":"Jiming Chen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jiming-chen.github.io/courses/cs4414/lecture02/"},"publisher":{"@type":"Organization","name":"Jiming Chen","logo":{"@type":"ImageObject","url":"https://jiming-chen.github.io/icons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jiming-chen.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://jiming-chen.github.io/icons/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jiming-chen.github.io/about title="About Me"><span>About Me</span></a></li><li><a href=https://jiming-chen.github.io/projects/ title=Portfolio><span>Portfolio</span></a></li><li><a href=https://jiming-chen.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://jiming-chen.github.io/courses/ title="Course Notes"><span>Course Notes</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Lecture 2: The Evolution and Architecture of Modern Computers</h1><div class=post-meta><span title='2025-08-28 15:09:04 -0400 EDT'>August 28, 2025</span>&nbsp;·&nbsp;Jiming Chen</div></header><div class=post-content><p>Modern CPU architecture has many cores on one chip, each of which has their own cache. There is also off-chip memory. Nvidia GPUs have many cores, and we can think about them virtually in 3D shapes (?? idk, Birman yaps).</p><p>Moore&rsquo;s Law says that because of transistor size decreases, we can get exponential speedups. But around 2006-2009, it seemed to be ending because faster chips require faster clock and can fry.</p><p>However, we started using parallelism (many CPUs with lower clock speeds).</p><p>A computer needs nearby memory, but applications need access to &ldquo;all&rdquo; the memory. This is why we get &ldquo;non-uniform memory access&rdquo; behavior (NUMA). With NUMA, we seem to have been able to keep up with Moore&rsquo;s Law.</p><p>Gene Ahmdahl researched parallelism and asked how fast we could do things with infinite parallelism. He realized that sequential bottlenecks become a huge issue, similar to how driving in a fast car doesn&rsquo;t matter if you&rsquo;re stuck behind a tractor. Ahmdal said that if $p$ is the percentage of the task that can be parallelized, then the maximum possible speed up is $1/(1-p)$ (this just says we cannot speed up sequential tasks).</p><p>When we write code with NUMA, nothing changes about what we actually write, but since each NUMA CPU (pair) has allocated DRAM, only a small portion of the memory is fast.</p><p>This results in a 4-tiered hierarchy for data access: registers, local DRAM, remote DRAM, and off-chip memory.</p><p>NUMA &ldquo;pretends&rdquo; to be old architecture by abstracting away architectural differences, but ML often does not know about this, resulting in unoptimized code. Similar behavior also occurs in GPUs such as Nvidia&rsquo;s H100.</p><p>Why are Python and Java expensive?</p><p>Python is interpreted and cpompiles down to a high-level representation. But this interpreter can make a lot of bad decisions for you.</p><p>Java compiles twice (to byte code then via JIT). This compiled code can be very fast, but sometimes, the compiler has to do a lot of type checking (at compile-time and runtime) because of dynamic types and polymorphism. Additionally, everything is an object, which hsa a lot of overhead.</p><p>In C++, all objects are compile-time, so there is no type-checking at runtime.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://jiming-chen.github.io/>Jiming Chen</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>